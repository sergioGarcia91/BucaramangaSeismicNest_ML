{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPwDJ5zP9zqfs/cTQ24kGiA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergioGarcia91/BucaramangaSeismicNest_ML/blob/main/ML_SismosNido_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The consultation was made in the Seismicity Catalog of the Servicio Geológico Colombiano: http://bdrsnc.sgc.gov.co/paginas1/catalogo/index.php\n",
        "\n",
        "Characteristics of the quadrant:\n",
        "\n",
        "| | Min | Max |\n",
        "|--------|-------|------|\n",
        "| Longitud | -73.4 | -72.8 |\n",
        "| Latitud | 6.5 | 7.1 |\n",
        "| Año | 1994 | 2023 |\n",
        "\n",
        "Subsequently, the events from January and February 2024 were considered to reevaluate the models, as for the month of December, the models indicated the possible occurrence of seismic events.\n"
      ],
      "metadata": {
        "id": "nyPBLpV9uFTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start"
      ],
      "metadata": {
        "id": "pvZ85sUsumIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FOPM6l-DvUYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install contextily"
      ],
      "metadata": {
        "id": "4LSJn0krOHe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gp\n",
        "import contextily as cx # For the basemap in GeoPandas\n",
        "import xyzservices.providers as xyz # To choose the basemap\n",
        "import time\n",
        "\n",
        "from scipy.spatial import distance\n",
        "from sklearn.cluster import DBSCAN\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression # For the Logistic Regression model\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # To evaluate the model\n",
        "from sklearn.neural_network import MLPClassifier # For the Neural Network\n",
        "from joblib import dump, load # Save the model\n",
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "tXd606evvalh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "x-K1HV2RGpIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pathDatos = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/Catalogos/'\n",
        "pathSaveFiguras = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/Figuras/'\n",
        "# Seismic Catalog: June 1, 1993 - February 28, 2018\n",
        "excelFechas_01 = ['1994', '1995', '1996', '1997', '1998', '1999',\n",
        "                  '2000', '2001', '2002', '2003', '2004', '2005',\n",
        "                  '2006', '2007', '2008', '2009', '2010', '2011',\n",
        "                  '2012', '2013', '2014', '2015', '2016', '2017',\n",
        "                  '2018a']\n",
        "\n",
        "# Seismic Catalog: March 1, 2018 to the present (2023)\n",
        "excelFechas_02 = ['2018b', '2019', '2020', '2021', '2022', '2023']"
      ],
      "metadata": {
        "id": "fnRLHueLu7cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5dV6QCluEAD"
      },
      "outputs": [],
      "source": [
        "# The one from 1994 is read first.\n",
        "df = pd.read_excel(pathDatos+'1994.xlsx', decimal=',')\n",
        "\n",
        "# The others are concatenated.\n",
        "for year in excelFechas_01[1:]:\n",
        "  df2 = pd.read_excel(pathDatos+year+'.xlsx', decimal=',')\n",
        "  df = pd.concat([df, df2])\n",
        "\n",
        "del(df2) # This temporary one is deleted.\n",
        "\n",
        "# The columns of interest are selected.\n",
        "df = df[['FECHA', 'HORA_UTC', 'LATITUD (grados)', 'LONGITUD (grados)',\n",
        "         'PROFUNDIDAD (Km)', 'MAGNITUD Ml', 'ERROR LATITUD (Km)',\n",
        "         'ERROR LONGITUD (Km)', 'ERROR PROFUNDIDAD (Km)']]\n",
        "\n",
        "# The first one from the second catalog is read.\n",
        "df2 = pd.read_excel(pathDatos+'2018b.xlsx', decimal=',')\n",
        "\n",
        "# The second catalog is concatenated.\n",
        "for year in excelFechas_02[1:]:\n",
        "  df3 = pd.read_excel(pathDatos+year+'.xlsx', decimal=',')\n",
        "  df2 = pd.concat([df2, df3])\n",
        "\n",
        "del(df3) # This one is deleted.\n",
        "\n",
        "# A Split is performed on the Date-Time of the second catalog so\n",
        "# that it matches the format of the first catalog.\n",
        "df2[['FECHA', 'HORA_UTC']] = df2['FECHA - HORA UTC'].str.split(' ', expand=True)\n",
        "\n",
        "# Only the relevant ones are selected.\n",
        "df2 = df2[['FECHA', 'HORA_UTC', 'LATITUD ()', 'LONGITUD ()', 'PROF. (Km)',\n",
        "           'MAGNITUD', 'ERROR LATITUD (Km)', 'ERROR LONGITUD (Km)', 'ERROR PROFUNDIDAD (Km)']]\n",
        "\n",
        "# The columns of the second catalog are renamed.\n",
        "df2.columns = ['FECHA', 'HORA_UTC', 'LATITUD (grados)', 'LONGITUD (grados)',\n",
        "               'PROFUNDIDAD (Km)', 'MAGNITUD Ml', 'ERROR LATITUD (Km)',\n",
        "               'ERROR LONGITUD (Km)', 'ERROR PROFUNDIDAD (Km)']\n",
        "\n",
        "df = pd.concat([df, df2])\n",
        "\n",
        "# Remove possible duplicates.\n",
        "df.drop_duplicates(inplace=True,\n",
        "                   ignore_index=True)\n",
        "df.reset_index(drop=True)\n",
        "\n",
        "del(df2) # The second catalog is deleted.\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "rRT3bdq5-cWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.round(df.describe(), 3)"
      ],
      "metadata": {
        "id": "spMkwGp9-ngJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graphs"
      ],
      "metadata": {
        "id": "SC1eBDFxFqKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histograms"
      ],
      "metadata": {
        "id": "Fp2gxq1r-9IZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['PROFUNDIDAD (Km)'],\n",
        "         bins=np.arange(0, 510, 10)) # Specify a bin width of 10 km for the bars.\n",
        "\n",
        "plt.xlabel('Depth [km]')\n",
        "plt.yscale('log') # To make the Y-axis logarithmic, in order to better see the contrasts.\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.xticks(np.arange(0, 510, 50))\n",
        "plt.xlim(-10, 510)\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Cantidad de eventos en profundidad 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'histEventos_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RKeQ6zZX-_Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "VUuwASZfHhqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['MAGNITUD Ml'],\n",
        "         bins=np.arange(0, 8, 0.5))\n",
        "\n",
        "plt.xlabel('Magnitude')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.xticks(np.arange(0, 8, 0.5))\n",
        "plt.xlim(-1, 8)\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Cantidad de eventos por Magnitud 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'histMagnitudEventos_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZvPwvyvgHgGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['LATITUD (grados)'],\n",
        "         bins=np.arange(6.5, 7.11, 0.025))\n",
        "\n",
        "plt.xlabel('Latitude [degrees]')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.xticks(np.arange(6.5, 7.11, 0.05))\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Cantidad de eventos en Latitud 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'histLatitudEventos_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e2Wk6-XP_v2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['LONGITUD (grados)'],\n",
        "         bins=np.arange(-73.4, -72.79, 0.025))\n",
        "\n",
        "plt.xlabel('Longitude [degrees]')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.xticks(np.arange(-73.4, -72.79, 0.05))\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Cantidad de eventos en Longitud')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'histLongitudEventos_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WrIma6ywA0To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scatter plots"
      ],
      "metadata": {
        "id": "iDVVwdiNBimd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "plt.scatter(df['LONGITUD (grados)'],\n",
        "            df['PROFUNDIDAD (Km)'],\n",
        "            s= 5,\n",
        "            c='r')\n",
        "\n",
        "plt.xlabel('Longitude [degrees]')\n",
        "plt.ylabel('Depth [km]')\n",
        "\n",
        "plt.yticks(np.arange(0, 460, 50))\n",
        "plt.ylim(-10, 480)\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Corte W-E 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'CorteWE_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YQmgRXEEBlCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "plt.scatter(df['LATITUD (grados)'],\n",
        "            df['PROFUNDIDAD (Km)'],\n",
        "            s= 5,\n",
        "            c='r')\n",
        "\n",
        "plt.xlabel('Latitude [degrees]')\n",
        "plt.ylabel('Depth [km]')\n",
        "\n",
        "plt.yticks(np.arange(0, 460, 50))\n",
        "plt.ylim(-10, 480)\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Corte S-N 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'CorteSN_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lobFWTNlCRII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "plt.scatter(df['LONGITUD (grados)'],\n",
        "            df['LATITUD (grados)'],\n",
        "            s= 5,\n",
        "            c='r')\n",
        "\n",
        "plt.xlabel('Latitude [degrees]')\n",
        "plt.ylabel('Depth [km]')\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Vista en Planta 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'CortePlanta_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1rkp5JGyDQ-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time vs. Magnitude (Ml)"
      ],
      "metadata": {
        "id": "vtNFDv7DDsv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date-Time'] = df['FECHA'] + ' ' + df['HORA_UTC']\n",
        "df['Date-Time'] = pd.to_datetime(df['Date-Time'], yearfirst=True)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "wqEhJkQCDt3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save/Load df"
      ],
      "metadata": {
        "id": "sDE9cm9ObS8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(pathDatos+'df.csv', index=False)"
      ],
      "metadata": {
        "id": "ezEJLYuybWO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(pathDatos+'df.csv')\n",
        "df['Date-Time'] = pd.to_datetime(df['Date-Time'], yearfirst=True)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "biKKo7OHbad3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "wBbrlvSrEyEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.scatter(df['Date-Time'],\n",
        "            df['MAGNITUD Ml'],\n",
        "            s=1,\n",
        "            label='Sismos',\n",
        "            alpha=0.5)\n",
        "\n",
        "Q1 = df['MAGNITUD Ml'].quantile(0.25)\n",
        "Q2 = df['MAGNITUD Ml'].quantile(0.5)\n",
        "Q3 = df['MAGNITUD Ml'].quantile(0.75)\n",
        "\n",
        "left, right = plt.xlim()\n",
        "\n",
        "plt.hlines(y= Q1,\n",
        "           xmin=left,\n",
        "           xmax=right,\n",
        "           ls='-',\n",
        "           color='b',\n",
        "           label='Q1={}'.format(Q1))\n",
        "\n",
        "plt.hlines(y= Q2,\n",
        "           xmin=left,\n",
        "           xmax=right,\n",
        "           ls='-',\n",
        "           color='y',\n",
        "           label='Q2={}'.format(Q2))\n",
        "\n",
        "plt.hlines(y= Q3,\n",
        "           xmin=left,\n",
        "           xmax=right,\n",
        "           ls='-',\n",
        "           color='m',\n",
        "           label='Q3={}'.format(Q3))\n",
        "\n",
        "\n",
        "plt.grid(ls='--', color='k', alpha=0.5)\n",
        "plt.xlim(left, right)\n",
        "plt.ylim(0,7)\n",
        "\n",
        "plt.xlabel('Date - UTC')\n",
        "plt.ylabel('Ml')\n",
        "plt.legend()\n",
        "\n",
        "plt.title('1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'tiempo_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6zihNMGwEevD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Depth >= 50 km"
      ],
      "metadata": {
        "id": "PzDvphVRGL8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_nido = df[df['PROFUNDIDAD (Km)'] >= 50].reset_index(drop=True)\n",
        "df_nido"
      ],
      "metadata": {
        "id": "HPC6ZrRpGQnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original number of events was 146408; by considering only those at depths greater than 50 km, a DataFrame with 145722 events is obtained. Approximately 0.5% (686 events) were removed."
      ],
      "metadata": {
        "id": "2J93UjxjIxNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df_nido['PROFUNDIDAD (Km)'],\n",
        "         bins=np.arange(0, 510, 10))\n",
        "\n",
        "plt.xlabel('Depth [km]')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.xticks(np.arange(0, 510, 50))\n",
        "plt.xlim(-10, 510)\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Cantidad de eventos en profundidad >50 km 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'histEventos_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BfWcIC5RHCFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df_nido['MAGNITUD Ml'],\n",
        "         bins=np.arange(0, 8, 0.25))\n",
        "\n",
        "plt.xlabel('Magnitude')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.xticks(np.arange(0, 8, 0.5))\n",
        "plt.xlim(-1, 8)\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Cantidad de eventos por Magnitud >50 km 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'histMagnitud_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IgfOIjU1H4NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.scatter(df_nido['Date-Time'],\n",
        "            df_nido['MAGNITUD Ml'],\n",
        "            s=1,\n",
        "            label='Sismos',\n",
        "            alpha=0.5)\n",
        "\n",
        "Q1 = df_nido['MAGNITUD Ml'].quantile(0.25)\n",
        "Q2 = df_nido['MAGNITUD Ml'].quantile(0.5)\n",
        "Q3 = df_nido['MAGNITUD Ml'].quantile(0.75)\n",
        "\n",
        "left, right = plt.xlim()\n",
        "\n",
        "plt.hlines(y= Q1,\n",
        "           xmin=left,\n",
        "           xmax=right,\n",
        "           ls='-',\n",
        "           color='b',\n",
        "           label='Q1={}'.format(Q1))\n",
        "\n",
        "plt.hlines(y= Q2,\n",
        "           xmin=left,\n",
        "           xmax=right,\n",
        "           ls='-',\n",
        "           color='y',\n",
        "           label='Q2={}'.format(Q2))\n",
        "\n",
        "plt.hlines(y= Q3,\n",
        "           xmin=left,\n",
        "           xmax=right,\n",
        "           ls='-',\n",
        "           color='m',\n",
        "           label='Q3={}'.format(Q3))\n",
        "\n",
        "\n",
        "plt.grid(ls='--', color='k', alpha=0.5)\n",
        "plt.xlim(left, right)\n",
        "plt.ylim(0,7)\n",
        "\n",
        "plt.xlabel('Date - UTC')\n",
        "plt.ylabel('Ml')\n",
        "plt.legend()\n",
        "plt.title('Eventos profundidades >50 km 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'tiempo_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "12H70Gk3GxXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there seems to be insufficient information about earthquakes with magnitudes lower than 2.5 before 2008, only events with magnitudes of 2.5 and above will be considered."
      ],
      "metadata": {
        "id": "Fj_uZ7NOHVQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ml >= 2.5"
      ],
      "metadata": {
        "id": "3Evu_EYDIc5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_interes = df_nido[df_nido['MAGNITUD Ml'] >= 2.5].reset_index(drop=True)\n",
        "df_interes"
      ],
      "metadata": {
        "id": "cX7BaPUNIf6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of the 145722 events at depths greater than 50 km, only 31822 events have magnitudes greater than or equal to 2.5. A total of 113900 events were removed, which corresponds to approximately 79% of the total events at these depths."
      ],
      "metadata": {
        "id": "STgddNUCJirm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df_interes['MAGNITUD Ml'],\n",
        "         bins=np.arange(0, 8, 0.25))\n",
        "\n",
        "plt.xlabel('Magnitude')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.xticks(np.arange(0, 8, 0.5))\n",
        "plt.xlim(-1, 8)\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Cantidad de eventos por Magnitud >=2.5, >50 km 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'histMagnitud_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_Xlt7SH4Kf8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.scatter(df_interes['Date-Time'],\n",
        "            df_interes['MAGNITUD Ml'],\n",
        "            s=1,\n",
        "            label='Sismos',\n",
        "            alpha=0.5)\n",
        "\n",
        "Q1 = df_interes['MAGNITUD Ml'].quantile(0.25)\n",
        "Q2 = df_interes['MAGNITUD Ml'].quantile(0.5)\n",
        "Q3 = df_interes['MAGNITUD Ml'].quantile(0.75)\n",
        "\n",
        "left, right = plt.xlim()\n",
        "\n",
        "plt.hlines(y= Q1,\n",
        "           xmin=left,\n",
        "           xmax=right,\n",
        "           ls='-',\n",
        "           color='b',\n",
        "           label='Q1={}'.format(Q1))\n",
        "\n",
        "plt.hlines(y= Q2,\n",
        "           xmin=left,\n",
        "           xmax=right,\n",
        "           ls='-',\n",
        "           color='y',\n",
        "           label='Q2={}'.format(Q2))\n",
        "\n",
        "plt.hlines(y= Q3,\n",
        "           xmin=left,\n",
        "           xmax=right,\n",
        "           ls='-',\n",
        "           color='m',\n",
        "           label='Q3={}'.format(Q3))\n",
        "\n",
        "\n",
        "plt.grid(ls='--', color='k', alpha=0.5)\n",
        "plt.xlim(left, right)\n",
        "plt.ylim(2,7)\n",
        "\n",
        "plt.xlabel('Date - UTC')\n",
        "plt.ylabel('Ml')\n",
        "plt.legend()\n",
        "plt.title('Eventos Magnitud >= 2.5, >50 km 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'tiempo_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vnpNU9m0Jgtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "years = np.arange(1994, 2024)\n",
        "eventsPerYear = []\n",
        "\n",
        "for year in years:\n",
        "  df_filtrado = df_interes[df_interes['Date-Time'].dt.year == year]\n",
        "  nE = len(df_filtrado)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(8,7))\n",
        "  ax.scatter(df_filtrado['LONGITUD (grados)'],\n",
        "             df_filtrado['LATITUD (grados)'],\n",
        "             s= 5, c='r')\n",
        "\n",
        "\n",
        "  ax.set_xlabel('Longitude [degrees]')\n",
        "  ax.set_ylabel('Latitude [degrees]')\n",
        "\n",
        "  ax.set_ylim(6.5, 7.1)\n",
        "  ax.set_xlim(-73.4, -72.8)\n",
        "  #ax.axis('equal')\n",
        "\n",
        "  cx.add_basemap(ax=ax,\n",
        "                 crs='epsg:4326',\n",
        "                 source=xyz.OpenTopoMap,\n",
        "                 reset_extent=True)\n",
        "\n",
        "  plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "  plt.title(f'Año {year}: {nE} eventos')\n",
        "\n",
        "  eventsPerYear.append(nE)\n",
        "\n",
        "  plt.savefig((pathSaveFiguras + f'mapa_{year}.png'),\n",
        "              format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "  plt.show()\n",
        "  print('\\n')\n",
        "\n",
        "del(nE, df_filtrado)"
      ],
      "metadata": {
        "id": "uvuPBV1WLiUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Año: Eventos')\n",
        "for y,e in zip(years, eventsPerYear):\n",
        "  print(f'{y}: {e}')"
      ],
      "metadata": {
        "id": "gM8F-nogRnnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.plot(years, eventsPerYear, ls='--')\n",
        "plt.scatter(years, eventsPerYear, c='r', s=10)\n",
        "\n",
        "plt.xlabel('Año')\n",
        "plt.ylabel('Cantidad')\n",
        "\n",
        "plt.xticks(np.arange(1994, 2025, 2))\n",
        "\n",
        "plt.title('Eventos por año Ml >=2.5, >50 km')\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'eventosPorYear_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mRZkNHrgSKF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In the maps, there is an alignment of seismic events during the years 1994 and 1995. What could be the explanation?\n",
        "\n",
        "- Apparently, over time, fewer seismic events have been recorded. Since 2008, this reduction seems to have been significant."
      ],
      "metadata": {
        "id": "bSQyfN-TT0sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Por día"
      ],
      "metadata": {
        "id": "RQwfUtmuXKeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_Dias = {'Dia':[],\n",
        "             '2.5-3.0':[],\n",
        "             '3.0-3.5':[],\n",
        "             '3.5-4.0':[],\n",
        "             '4.0-4.5':[],\n",
        "             '4.5-5.0':[],\n",
        "             '5.0-5.5':[],\n",
        "             '5.5-':[],}\n",
        "\n",
        "# It seems that not every day there is a record of events at depths greater\n",
        "# than 50 km and magnitudes greater than 2.5.\n",
        "\n",
        "inicio = np.datetime64('1994-01-01')\n",
        "fin = np.datetime64('2024-01-01')\n",
        "array_fechas = np.arange(inicio, fin, dtype='datetime64[D]')\n",
        "\n",
        "for date in array_fechas:\n",
        "  df_filtrado = df_interes[df_interes['FECHA'] == str(date)]\n",
        "  dict_Dias['Dia'].append(str(date))\n",
        "\n",
        "  for ml in np.arange(2.5, 6.0, 0.5):\n",
        "    if ml == 5.5:\n",
        "      nE = df_filtrado['MAGNITUD Ml'] >= 5.5\n",
        "\n",
        "      dict_Dias['5.5-'].append(sum(nE))\n",
        "\n",
        "    else:\n",
        "      nE = (df_filtrado['MAGNITUD Ml'] >= ml) & (df_filtrado['MAGNITUD Ml'] < (ml+0.5) )\n",
        "      strDict = f'{ml}-{ml+0.5}'\n",
        "\n",
        "      dict_Dias[strDict].append(sum(nE))\n",
        "\n"
      ],
      "metadata": {
        "id": "uMdGaMsEXNgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias = pd.DataFrame.from_dict(dict_Dias)\n",
        "df_Dias['Total dia'] = df_Dias.sum(axis=1)\n",
        "df_Dias"
      ],
      "metadata": {
        "id": "dS6OZ9Y1ZaZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It should sum to 31822.\n",
        "df_Dias['Total dia'].sum()"
      ],
      "metadata": {
        "id": "7_-n2r2phSJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering events at depths greater than 50 km and magnitudes greater than 2.5, some days do not record events under this condition."
      ],
      "metadata": {
        "id": "2V5kmmzaeW6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save/Load CSV Days"
      ],
      "metadata": {
        "id": "_LkFGPC_jjNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias.to_csv(pathDatos+'df_Dias.csv', index=False)"
      ],
      "metadata": {
        "id": "bu-4ndq-taAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias = pd.read_csv(pathDatos+'df_Dias.csv')\n",
        "df_Dias['Fecha'] = pd.to_datetime(df_Dias['Dia'], yearfirst=True)\n",
        "df_Dias"
      ],
      "metadata": {
        "id": "wJLImnOejkwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "\n",
        "for i in df_Dias.columns[1:-2]:\n",
        "  filtro = df_Dias[i] > 0\n",
        "\n",
        "  plt.scatter(df_Dias['Fecha'][filtro],\n",
        "              df_Dias[i][filtro],\n",
        "              s=5,\n",
        "              label=i)\n",
        "\n",
        "plt.grid(ls='--', color='k', alpha=0.5)\n",
        "\n",
        "plt.xlabel('Date - UTC')\n",
        "plt.ylabel('Cantidad')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fwIVYeHbf1Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semivariogram over Time"
      ],
      "metadata": {
        "id": "DQo1lxxwl_N_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A \"temporal\" semivariance analysis will be performed, comparing the number of events per day with differences of up to one year. This comparison will be carried out in seven different subgroups of seismic events, which were divided according to magnitudes of 2.5, with intervals of 0.5 between each subgroup."
      ],
      "metadata": {
        "id": "j1Xb5fcAzugM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lags = np.arange(1, 366, 10) #A year or 365 days will be considered.\n",
        "lags\n",
        "semiVariograma = []\n",
        "\n",
        "for day in lags:\n",
        "  # To know which day it falls on.\n",
        "  print(day)\n",
        "  # To sum the semivariance and then average it.\n",
        "  var = np.zeros(7)\n",
        "  # To know how many data points the semivariance is averaged over at the end.\n",
        "  nDatos = 0\n",
        "  for index in np.arange(len(df_Dias)):\n",
        "    # The data to be compared with the others\n",
        "    DatoLag = df_Dias[['2.5-3.0', '3.0-3.5', '3.5-4.0', '4.0-4.5', '4.5-5.0', '5.0-5.5', '5.5-']].iloc[index]\n",
        "    # The rest of the data with which it will be compared\n",
        "    DatosLags = df_Dias[['2.5-3.0', '3.0-3.5', '3.5-4.0', '4.0-4.5', '4.5-5.0', '5.0-5.5', '5.5-']].iloc[index+1::day]\n",
        "    # The semivariance is calculated\n",
        "    semiVar = ((DatosLags - DatoLag)**2)/2\n",
        "    # It is added to the empty array\n",
        "    var = var + semiVar.sum()\n",
        "    # The count of how many data points are being summed is kept track of\n",
        "    nDatos += semiVar.shape[0]\n",
        "  # The semivariance is averaged\n",
        "  var2 = np.array(var/nDatos)\n",
        "  # It is included to create the semivariograms\n",
        "  # The total number of rows is equal to the number of days\n",
        "  semiVariograma.append(list(var2))\n",
        "semiVariograma = np.array(semiVariograma)"
      ],
      "metadata": {
        "id": "k_dbewSGf6AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_semiVar = np.concatenate((np.reshape(lags, (-1, 1)), semiVariograma) ,axis=1)\n",
        "df_semiVar = pd.DataFrame(df_semiVar, columns=['Lag dias', '2.5-3.0', '3.0-3.5', '3.5-4.0', '4.0-4.5', '4.5-5.0', '5.0-5.5', '5.5-'])\n",
        "df_semiVar"
      ],
      "metadata": {
        "id": "FLVcfCn50n1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save/Load CSV Semivariance"
      ],
      "metadata": {
        "id": "hnGQLZO00d-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_semiVar.to_csv(pathDatos+'df_semiVar.csv', index=False)"
      ],
      "metadata": {
        "id": "27BLFQvntW2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_semiVar = pd.read_csv(pathDatos+'df_semiVar.csv')\n",
        "df_semiVar"
      ],
      "metadata": {
        "id": "Qfk85MHFuTRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot semivariogram"
      ],
      "metadata": {
        "id": "-iVp_Sh48Ndr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_semiVar.columns"
      ],
      "metadata": {
        "id": "haxtRaAd8V3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias.var()"
      ],
      "metadata": {
        "id": "dfMkeZwA-sed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for subgrupo in df_semiVar.columns[1:]:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.plot(df_semiVar['Lag dias'],\n",
        "           df_semiVar[subgrupo], ls='--')\n",
        "  plt.scatter(df_semiVar['Lag dias'],\n",
        "              df_semiVar[subgrupo],\n",
        "              c='k',\n",
        "              s=10)\n",
        "  plt.axhline(y=df_Dias[subgrupo].var(), color='r', linestyle='--', label='Varianza')\n",
        "\n",
        "  plt.legend()\n",
        "  plt.title('Subgrupo Ml: ' + subgrupo)\n",
        "  plt.xlabel('Lag [días]')\n",
        "  plt.ylabel('Semivarianza')\n",
        "  plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "\n",
        "  plt.savefig((pathSaveFiguras + f'semiVar_{subgrupo}.png'),\n",
        "              format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "  plt.show()\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "tUqbSAq1oeJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "for subgrupo in df_semiVar.columns[1:]:\n",
        "  plt.plot(df_semiVar['Lag dias'],\n",
        "           df_semiVar[subgrupo],\n",
        "           ls='--',\n",
        "           label=subgrupo)\n",
        "  plt.scatter(df_semiVar['Lag dias'],\n",
        "              df_semiVar[subgrupo],\n",
        "              s=10)\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('Lag [días]')\n",
        "plt.ylabel('Semivarianza')\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'semivariogramas_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Xvfrti1G-DmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias.columns"
      ],
      "metadata": {
        "id": "iadpl7v0kPEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for subgrupo in df_Dias.columns[1:8]:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  #plt.plot(df_Dias['Fecha'],\n",
        "  #         df_Dias[subgrupo], ls='--')\n",
        "  plt.scatter(df_Dias['Fecha'],\n",
        "              df_Dias[subgrupo],\n",
        "              c='b',\n",
        "              s=3)\n",
        "  varianzaML = np.round(df_Dias[subgrupo].var(), 3)\n",
        "  plt.axhline(y=varianzaML, color='r', linestyle='--', label=f'Varianza: {varianzaML}')\n",
        "\n",
        "  plt.legend()\n",
        "  plt.title('Subgrupo Ml: ' + subgrupo)\n",
        "  plt.xlabel('Fecha - UTC')\n",
        "  plt.ylabel('Cantidad eventos')\n",
        "  plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "\n",
        "  plt.show()\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "jsQf_N4ykM6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.round(df_Dias.describe(), 2)"
      ],
      "metadata": {
        "id": "Jl5Qz-tDrxY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of the temporal semivariogram show that lower magnitudes exhibit higher semivariance, which is understandable given that these are the more frequent events. In comparison, higher-magnitude events, which are less frequent, show lower semivariance, as generally few or no events are recorded, especially those with magnitudes greater than 5.\n",
        "\n",
        "By reviewing the scatter plot and the variance of each subgroup, part of the previous conclusion can be corroborated. Additionally, from the `3.5-4.0` subgroup onward, it is observed that 75% of the data are 0, indicating that there is no record of earthquakes with those magnitudes for the majority."
      ],
      "metadata": {
        "id": "nTiuqjkW-mdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distance"
      ],
      "metadata": {
        "id": "zvcI_inynvQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering that each day is represented as a vector, with the coordinates or axes corresponding to each of the columns representing the number of events by magnitude subgroup, the distance between all the days will be calculated to assess how close they may be to each other and determine if there are similarities between the events."
      ],
      "metadata": {
        "id": "Y2FK_L0Enx2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectoresDias = df_Dias.iloc[:,1:8].to_numpy()\n",
        "vectoresDias"
      ],
      "metadata": {
        "id": "BPbrls3WofKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distanciaDias = distance.cdist(vectoresDias, vectoresDias, 'euclidean')\n",
        "distanciaDias.shape # The distance matrix between all the days."
      ],
      "metadata": {
        "id": "Eps0d4T6nxlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The diagonal is zero because it represents the distance of that day to itself,\n",
        "# and it is symmetric, so we are only interested in the upper or lower triangle\n",
        "# of the matrix, excluding the diagonal.\n",
        "\n",
        "np.round(distanciaDias[0:5,0:5], 2) # Only view 5 data points out of the 10957."
      ],
      "metadata": {
        "id": "EiF2658zo1nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will look for the indices of the numbers located above the diagonal.\n",
        "# Note: The diagonal will not be considered.\n",
        "\n",
        "iu = np.triu_indices(distanciaDias.shape[0],\n",
        "                     1) # k=0 to start from the diagonal; k=1 to move 1 position up.\n",
        "iu"
      ],
      "metadata": {
        "id": "_G18SOHrqOX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We pass the indices to select only the upper part.\n",
        "distDias_vector = distanciaDias[iu]\n",
        "distDias_vector"
      ],
      "metadata": {
        "id": "JshYE6mvqVV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' Min.: ', np.min(distDias_vector))\n",
        "print(' Max.: ', np.max(distDias_vector))\n",
        "print(' Media: ', np.mean(distDias_vector))\n",
        "print(' Mediana: ', np.median(distDias_vector))"
      ],
      "metadata": {
        "id": "zDCDLk4TrsJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.linspace(0, 13, 14)"
      ],
      "metadata": {
        "id": "EEkgy41Vte2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(distDias_vector,\n",
        "         bins=np.linspace(0, 13, 14*5))\n",
        "\n",
        "plt.xlabel('Distancia')\n",
        "#plt.yscale('log')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Distancia entre los vectores Días 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'distancia_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EyJFeX40s5WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(distDias_vector,\n",
        "         bins=np.linspace(0, 13, 14*5))\n",
        "\n",
        "plt.xlabel('Distancia')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Distancia entre los vectores Días 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'distanciaLog_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Svaf7IzVwQIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "plt.boxplot(distDias_vector)\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "plt.title('Distancia entre los vectores Días 1994-2023')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'boxplot_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iPgH4GOQv6Va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1, Q2, Q3, P90 = np.quantile(distDias_vector, [0.25, 0.50, 0.75, 0.90])\n",
        "RIC = Q3-Q1\n",
        "minOutliers = Q1 - 1.5*RIC\n",
        "maxOutliers = Q3 + 1.5*RIC\n",
        "countOutliers = sum(distDias_vector > maxOutliers)\n",
        "porcentajeOutliers = np.round(countOutliers*100/len(distDias_vector), 2)\n",
        "print('Min. Outliers: ', minOutliers)\n",
        "print('Q1: ', Q1)\n",
        "print('Q2: ', Q2)\n",
        "print('Q3: ', Q3)\n",
        "print('P90: ', P90)\n",
        "print('Max. Outliers: ', maxOutliers)\n",
        "print(f'Cantidad Outliers: {countOutliers} ({porcentajeOutliers}%)')"
      ],
      "metadata": {
        "id": "WYYEbotiwogg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBSCAN"
      ],
      "metadata": {
        "id": "5b15cRh7vx2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering the information from the quartiles and what is observed in the histograms, the distance values ​​will be adjusted and their behavior evaluated. Later, the days will be visualized as a function of time."
      ],
      "metadata": {
        "id": "htYT8HXOzOp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectoresDias.shape)\n",
        "vectoresDias # The day vectors to cluster"
      ],
      "metadata": {
        "id": "8OocBT5n0ZMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tinicio = time.perf_counter()\n",
        "\n",
        "distanciaMax = 0.9\n",
        "eventos = 100\n",
        "\n",
        "modelo_DBSCAN = DBSCAN(eps=distanciaMax, min_samples=eventos, metric='euclidean').fit(vectoresDias)\n",
        "\n",
        "tfinal = time.perf_counter()\n",
        "tiempo = tfinal - tinicio\n",
        "\n",
        "print('Segundos empleados: ',tiempo)"
      ],
      "metadata": {
        "id": "3CZs4z0NzPPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cluster info\n",
        "clusters = Counter(modelo_DBSCAN.labels_)\n",
        "print(clusters)\n",
        "print(df_Dias[modelo_DBSCAN.labels_ == -1].head())\n",
        "print('Total clusters = {}'.format(len(clusters)-1))\n",
        "print('Total clusters events = ', len(df_Dias[modelo_DBSCAN.labels_ != -1]))"
      ],
      "metadata": {
        "id": "7I4PB9tU0vIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the outliers\n",
        "outliers_df2 = df_Dias[modelo_DBSCAN.labels_ == -1]\n",
        "# The rest of the data\n",
        "cluster_df2 = df_Dias[modelo_DBSCAN.labels_ != -1]\n",
        "\n",
        "# Colors for the clusters\n",
        "colors2 = modelo_DBSCAN.labels_\n",
        "color_cluster2 = colors2[colors2 != -1]\n",
        "\n",
        "porcentajeOutliers = np.round((len(outliers_df2) / len(modelo_DBSCAN.labels_)) *100, 2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,7))\n",
        "\n",
        "ax.scatter(outliers_df2['Fecha'], outliers_df2['Total dia'],\n",
        "           s=3, c='r', label=f'{len(outliers_df2)} Outliers ({porcentajeOutliers}%)', alpha=0.7)\n",
        "\n",
        "ax.scatter(cluster_df2['Fecha'], cluster_df2['Total dia'],\n",
        "           s=3, c=color_cluster2, alpha=0.5)\n",
        "\n",
        "ax.set_ylabel('Evento totales en el dia')\n",
        "ax.set_xlabel('Fecha - UTC')\n",
        "plt.title(f'Clusters: {len(clusters)-1}')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'DBSACAN_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TotCCZ5S1-DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers_df2 = df_Dias[modelo_DBSCAN.labels_ == -1]\n",
        "\n",
        "cluster_df2 = df_Dias[modelo_DBSCAN.labels_ != -1]\n",
        "\n",
        "colors2 = modelo_DBSCAN.labels_\n",
        "color_cluster2 = colors2[colors2 != -1]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,7))\n",
        "\n",
        "ax.scatter(cluster_df2['Fecha'], cluster_df2['Total dia'],\n",
        "           s=3, c=color_cluster2, alpha=0.5)\n",
        "\n",
        "ax.set_ylabel('Evento totales en el dia')\n",
        "ax.set_xlabel('Fecha - UTC')\n",
        "plt.title(f'Clusters: {len(clusters)-1}, sin Outliers')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'DBSCAN_sinOutliers_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TXTm_kVY5vMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusters.keys()"
      ],
      "metadata": {
        "id": "gqI5SudE6wi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusters.values()"
      ],
      "metadata": {
        "id": "zEGX8aM761qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.bar(clusters.keys(),\n",
        "        clusters.values())\n",
        "\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "\n",
        "plt.title('Eventos por Cluster')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'DBSCAN_hist_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D7RnWWk454y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apparently, clusters are considered to correspond to a maximum of 5 events per day, considering an `eps=0.9` and `min_samples=100`. Generating a total of 21 Clusters without including the outliers."
      ],
      "metadata": {
        "id": "qMRPInk48H4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "9i7ZAgpI-Bya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A principal component analysis (PCA) will be performed to examine how the 7 axes or magnitude subgroups defined for each day vector are affected, and to determine if there is any relationship between them."
      ],
      "metadata": {
        "id": "JCOSObXh-DUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_PCA = PCA(n_components=7) # That preserves the 7 components\n",
        "modelo_PCA.fit(vectoresDias)"
      ],
      "metadata": {
        "id": "SgJ8fbA--Kkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of variance explained for each of the 7 components\n",
        "modelo_PCA.explained_variance_ratio_ * 100"
      ],
      "metadata": {
        "id": "iYgZ0Nne-9vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first three components appear to account for approximately 60%, 20% and 9%, respectively."
      ],
      "metadata": {
        "id": "u8kreFMd_ROl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We transform to obtain the new 7 axes\n",
        "vectoresDias_PCA = modelo_PCA.transform(vectoresDias)\n",
        "np.round(vectoresDias_PCA[0:5,:], 2)"
      ],
      "metadata": {
        "id": "JouYzX2L-q5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter graph will be made considering only the first two axes."
      ],
      "metadata": {
        "id": "iHstEvAf_1Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "plt.scatter(vectoresDias_PCA[:,0],\n",
        "            vectoresDias_PCA[:,1],\n",
        "            s= 5)\n",
        "\n",
        "plt.xlabel('PC-1')\n",
        "plt.ylabel('PC-2')\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "\n",
        "plt.title('PCA')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2FBmxRLV_-HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "h = plt.hist2d(vectoresDias_PCA[:,0],\n",
        "           vectoresDias_PCA[:,1],\n",
        "           bins=60,\n",
        "           cmap='viridis',\n",
        "               cmin=1)\n",
        "\n",
        "plt.colorbar()\n",
        "\n",
        "plt.xlabel('PC-1')\n",
        "plt.ylabel('PC-2')\n",
        "plt.title('Hist-2D PCA')\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'PCA_01_02_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8GdwOvNWI-de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "h = plt.hist2d(vectoresDias_PCA[:,0],\n",
        "           vectoresDias_PCA[:,2],\n",
        "           bins=60,\n",
        "           cmap='viridis',\n",
        "               cmin=1)\n",
        "\n",
        "plt.colorbar()\n",
        "\n",
        "plt.xlabel('PC-1')\n",
        "plt.ylabel('PC-3')\n",
        "plt.title('Hist-2D PCA')\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'PCA_01_03_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VNFDKyPMOfdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "h = plt.hist2d(vectoresDias_PCA[:,1],\n",
        "           vectoresDias_PCA[:,2],\n",
        "           bins=60,\n",
        "           cmap='viridis',\n",
        "               cmin=1)\n",
        "\n",
        "plt.colorbar()\n",
        "\n",
        "plt.xlabel('PC-2')\n",
        "plt.ylabel('PC-3')\n",
        "plt.title('Hist-2D PCA')\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'PCA_02_03_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uLZ_jlJcOig5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN-PCA"
      ],
      "metadata": {
        "id": "H4WBugbDO2BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum(modelo_PCA.explained_variance_ratio_ [0:3]*100)"
      ],
      "metadata": {
        "id": "oSz5uv1xPAI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering that the first three principal components best represent the variance of the day vectors, a DBSCAN model will be generated with these components."
      ],
      "metadata": {
        "id": "rBRQen-DO51w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.round(vectoresDias_PCA[0:3], 2)"
      ],
      "metadata": {
        "id": "GTgmRb27Pmsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.round(vectoresDias_PCA[:,0:3], 2)"
      ],
      "metadata": {
        "id": "Wz94AIYGPdpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tinicio = time.perf_counter()\n",
        "\n",
        "distanciaMax = 0.9\n",
        "eventos = 500\n",
        "\n",
        "modelo_DBSCAN_PCA = DBSCAN(eps=distanciaMax, min_samples=eventos, metric='euclidean').fit(vectoresDias_PCA[:,0:3])\n",
        "\n",
        "tfinal = time.perf_counter()\n",
        "tiempo = tfinal - tinicio\n",
        "\n",
        "print('Segundos empleados: ',tiempo)"
      ],
      "metadata": {
        "id": "zRIes7NyO5Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cluster info\n",
        "clusters_PCA = Counter(modelo_DBSCAN_PCA.labels_)\n",
        "print(clusters_PCA)\n",
        "print(df_Dias[modelo_DBSCAN_PCA.labels_ == -1].head())\n",
        "print('Total clusters = {}'.format(len(clusters_PCA)-1))\n",
        "print('Total clusters events = ', len(df_Dias[modelo_DBSCAN_PCA.labels_ != -1]))"
      ],
      "metadata": {
        "id": "SIHlZteIPyBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "plt.scatter(vectoresDias_PCA[:,0],\n",
        "            vectoresDias_PCA[:,1],\n",
        "            s= 5,\n",
        "            c=modelo_DBSCAN_PCA.labels_)\n",
        "\n",
        "plt.xlabel('PC-1')\n",
        "plt.ylabel('PC-2')\n",
        "\n",
        "plt.grid(color='grey', ls='--', alpha=0.5)\n",
        "\n",
        "plt.title('PCA-DBSCAN')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'PCA_DBSCAN_01_02_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zSL18V8hP_LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "outliers_df2_PCA = df_Dias[modelo_DBSCAN_PCA.labels_ == -1]\n",
        "\n",
        "cluster_df2_PCA = df_Dias[modelo_DBSCAN_PCA.labels_ != -1]\n",
        "\n",
        "\n",
        "colors2_PCA = modelo_DBSCAN_PCA.labels_\n",
        "color_cluster2_PCA = colors2_PCA[colors2_PCA != -1]\n",
        "\n",
        "porcentajeOutliers_PCA = np.round((len(outliers_df2_PCA) / len(modelo_DBSCAN_PCA.labels_)) *100, 2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,7))\n",
        "\n",
        "ax.scatter(outliers_df2_PCA['Fecha'], outliers_df2_PCA['Total dia'],\n",
        "           s=3, c='r', label=f'{len(outliers_df2_PCA)} Outliers ({porcentajeOutliers_PCA}%)', alpha=0.3)\n",
        "\n",
        "ax.scatter(cluster_df2_PCA['Fecha'], cluster_df2_PCA['Total dia'],\n",
        "           s=3, c=color_cluster2_PCA, alpha=0.5)\n",
        "\n",
        "ax.set_ylabel('Evento totales en el dia')\n",
        "ax.set_xlabel('Fecha - UTC')\n",
        "plt.title(f'PCA-DBSCAN Clusters: {len(clusters_PCA)-1}')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'tiempo_PCA_DBSCAN_01_02_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "votS7MLLQrqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "outliers_df2_PCA = df_Dias[modelo_DBSCAN_PCA.labels_ == -1]\n",
        "\n",
        "cluster_df2_PCA = df_Dias[modelo_DBSCAN_PCA.labels_ != -1]\n",
        "\n",
        "\n",
        "colors2_PCA = modelo_DBSCAN_PCA.labels_\n",
        "color_cluster2_PCA = colors2_PCA[colors2_PCA != -1]\n",
        "\n",
        "porcentajeOutliers_PCA = np.round((len(outliers_df2_PCA) / len(modelo_DBSCAN_PCA.labels_)) *100, 2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,7))\n",
        "\n",
        "ax.scatter(cluster_df2_PCA['Fecha'], cluster_df2_PCA['Total dia'],\n",
        "           s=3, c=color_cluster2_PCA, alpha=1)\n",
        "\n",
        "ax.set_ylabel('Evento totales en el dia')\n",
        "ax.set_xlabel('Fecha - UTC')\n",
        "plt.title(f'PCA-DBSCAN Clusters: {len(clusters_PCA)-1}, sin Outliers')\n",
        "\n",
        "plt.savefig((pathSaveFiguras + 'tiempo_PCA_DBSCAN_sinOutliers_01_02_ml2.5_50km_1994_2023.png'),\n",
        "            format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ngns5v9gRMd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apparently, the results may be similar between both DBSCAN models."
      ],
      "metadata": {
        "id": "f3Orl_TfSR0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Model"
      ],
      "metadata": {
        "id": "wk4C6CKaSdwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A logistic regression model will be created in which we will try to predict the occurrence of events with magnitudes equal to or greater than 4.5. To do this, the 30 vectors of previous days will be taken into account to evaluate the possibility of an event of this magnitude occurring in the next 6 days.\n",
        "\n",
        "---\n",
        "Example:\n",
        "Today is January 30, so the input data would be the events recorded from January 1 to 30 (up to the current date). The model will then calculate the probability of an event with magnitude >= 4.5 occurring during the period from January 31 to February 5 (a total of 6 days)."
      ],
      "metadata": {
        "id": "PrLJT-rtTUUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias.sum(axis=0)"
      ],
      "metadata": {
        "id": "ZmmUzjwZSeNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add the 3 columns of interest\n",
        "df_Dias['Y'] = df_Dias[['4.5-5.0', '5.0-5.5', '5.5-']].sum(axis=1)\n",
        "np.round(df_Dias.describe())"
      ],
      "metadata": {
        "id": "zt6vm-hPTTOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias.head(10)"
      ],
      "metadata": {
        "id": "4ljDq3idUhTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias['Y'].sum() # A total of 497 earthquakes have been recorded with M>=4.5"
      ],
      "metadata": {
        "id": "6JdVjNXkU17j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias"
      ],
      "metadata": {
        "id": "e8sE6an1VGDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias[0:-2]"
      ],
      "metadata": {
        "id": "H5b3X764VM1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_Dias)"
      ],
      "metadata": {
        "id": "recvWQ72Xhxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear y/X"
      ],
      "metadata": {
        "id": "0CT_Ort4YGDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = []\n",
        "X = []\n",
        "\n",
        "dias_Considerar = 30\n",
        "\n",
        "for dias in range(dias_Considerar-1, len(df_Dias)-6):\n",
        "  di_ = dias_Considerar-1\n",
        "  datos = df_Dias[['2.5-3.0', '3.0-3.5', '3.5-4.0', '4.0-4.5', '4.5-5.0', '5.0-5.5', '5.5-']].iloc[dias-di_:dias+1].to_numpy()\n",
        "  #print(datos.shape)\n",
        "  datos = np.reshape(datos, (1,-1))\n",
        "  #print(datos.shape)\n",
        "  X.append(datos.tolist()[0])\n",
        "\n",
        "  # If day 30 is not included in the query to consider in the targets\n",
        "  # in training the model does not predict\n",
        "  # why?\n",
        "  # SD30 ... without day 30\n",
        "  # SD60 ... without day 60\n",
        "  target = df_Dias['Y'].iloc[dias+1:dias+7].sum()\n",
        "\n",
        "  # If day 30 is included, the model predicts better\n",
        "  # why?\n",
        "  # CD30 ... with day 30\n",
        "  # CD60 ... with day 60\n",
        "  #target = df_Dias['Y'].iloc[dias:dias+7].sum()\n",
        "\n",
        "  if target > 0 :\n",
        "    y.append(1)\n",
        "  else:\n",
        "    y.append(0)\n",
        "y = np.reshape(np.array(y), (-1,1))\n",
        "X = np.array(X)"
      ],
      "metadata": {
        "id": "XruWOd5_Vagy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "j-QleSo-Z65P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "sYYB0H-6ZYJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_RegLog = pd.DataFrame(np.concatenate((X,y), axis=1))\n",
        "df_RegLog"
      ],
      "metadata": {
        "id": "7yMJigE1XvHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save/Load CSV Reg Log"
      ],
      "metadata": {
        "id": "l7YqJsqmbzFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_RegLog.to_csv(pathDatos+'df_RegLog.csv', index=False)"
      ],
      "metadata": {
        "id": "FScK-IOmtOXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_RegLog = pd.read_csv(pathDatos+'df_RegLog.csv')\n",
        "df_RegLog"
      ],
      "metadata": {
        "id": "QSuBnvcyb4l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split"
      ],
      "metadata": {
        "id": "txHRDcC7mPuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model appears to improve when only earthquakes since 2008 are taken into account, but it is still not fully adequate for prediction, as it only manages to get it right in about 11 out of 70 cases. This is similar to the results obtained when using all data before 2023."
      ],
      "metadata": {
        "id": "ysh7GZiTr0Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The data before 2023 will be used for training\n",
        "# and the data from 2023 will be used for prediction\n",
        "filtro2022 = (df_Dias['Fecha'].iloc[dias_Considerar-1:-6].dt.year < 2023).to_numpy()\n",
        "\n",
        "# To see if it only improves since 2008\n",
        "# it doesn't seem to improve\n",
        "#l2008 = df_Dias['Fecha'].iloc[dias_Considerar-1:-6].dt.year >= 2008\n",
        "#l2023 = df_Dias['Fecha'].iloc[dias_Considerar-1:-6].dt.year < 2023\n",
        "#filtro2022 = (l2008 & l2023).to_numpy()\n",
        "\n",
        "filtro2023 = (df_Dias['Fecha'].iloc[dias_Considerar-1:-6].dt.year == 2023).to_numpy()\n",
        "\n",
        "y2022 = df_RegLog.iloc[:,-1][filtro2022].to_numpy()\n",
        "X2022 = df_RegLog.iloc[:,:-1][filtro2022].to_numpy()\n",
        "\n",
        "y2023 = df_RegLog.iloc[:,-1][filtro2023].to_numpy()\n",
        "X2023 = df_RegLog.iloc[:,:-1][filtro2023].to_numpy()\n"
      ],
      "metadata": {
        "id": "QDRkDa5HWR95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Numpy D30/D60"
      ],
      "metadata": {
        "id": "q9RCuUzk5pLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "prefijo = pathDatos + 'SD' + str(dias_Considerar)\n",
        "print(prefijo)\n",
        "np.savetxt(f'{prefijo}_y2022.txt', y2022)\n",
        "np.savetxt(f'{prefijo}_X2022.txt', X2022)\n",
        "np.savetxt(f'{prefijo}_y2023.txt', y2023)\n",
        "np.savetxt(f'{prefijo}_X2023.txt', X2023)\n",
        "'''"
      ],
      "metadata": {
        "id": "fb_8Z4Lk5ooh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y2022"
      ],
      "metadata": {
        "id": "wXxPYPOrcju5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2022"
      ],
      "metadata": {
        "id": "zvi5CHVock7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y2023"
      ],
      "metadata": {
        "id": "uIXMJ_xwcnQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2023"
      ],
      "metadata": {
        "id": "UfHPeAz8c7lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "FPf05IcBdJV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create the Logistic Regression model\n",
        "modelo_RegLog = LogisticRegression(max_iter=400)\n",
        "\n",
        "# Train the model\n",
        "modelo_RegLog.fit(X2022, y2022) # It seems that the iteration limit has been reached"
      ],
      "metadata": {
        "id": "TJSIER4ZdJHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saveRegLogo = False\n",
        "if saveRegLogo:\n",
        "\n",
        "  pathSave = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/ModelosMLP_Class/'\n",
        "\n",
        "  scoreRegLog = modelo_RegLog.score(X2022, y2022)\n",
        "  print(scoreRegLog)\n",
        "\n",
        "\n",
        "  Name = 'SD30_RegLog_scr' + str(round(scoreRegLog, 3)) + '.joblib'\n",
        "  dump(modelo_RegLog, pathSave+Name)\n",
        "  print(Name)"
      ],
      "metadata": {
        "id": "ec_aoalRrT6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathSave = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/ModelosMLP_Class/'\n",
        "#Name = 'SD30_RegLog_scr0.788.joblib'\n",
        "#modelo_RegLog = load(pathSave+Name)\n",
        "modelo_RegLog"
      ],
      "metadata": {
        "id": "tc5hCv__r50_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "kD9kT0Hzf_yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To predict label 0 or 1\n",
        "pred_RegLog = modelo_RegLog.predict(X2022)\n",
        "pred_RegLog"
      ],
      "metadata": {
        "id": "4wAPjRCRd0y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To know the probability of each class\n",
        "modelo_RegLog.predict_proba(X2022)"
      ],
      "metadata": {
        "id": "OKcbuzBWd_Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_RegLog.classes_ # in position 1 is the one of interest"
      ],
      "metadata": {
        "id": "MdoIT0neeL76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_RegLog_prob = modelo_RegLog.predict_proba(X2022)[:,1]\n",
        "pred_RegLog_prob"
      ],
      "metadata": {
        "id": "WweTqMtzeb8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nMin = dias_Considerar -1\n",
        "\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(15,5))\n",
        "\n",
        "ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2022],\n",
        "            pred_RegLog_prob*100,\n",
        "            s= 5,\n",
        "            c=pred_RegLog_prob*100,\n",
        "            cmap='YlGnBu',\n",
        "            vmin=0,\n",
        "            vmax=100)\n",
        "\n",
        "ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2022][y2022 >= 1],\n",
        "            y2022[y2022 >= 1] * 50,\n",
        "            s= 5,\n",
        "            c='r')\n",
        "\n",
        "\n",
        "ax1.set_ylabel('Probabilidad [%]')\n",
        "ax1.set_xlabel('Fecha - UTC')\n",
        "ax1.grid(ls='--', color='grey')\n",
        "\n",
        "plt.suptitle('1994-2022')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-8oyK518ixn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y2022, pred_RegLog).ravel()\n",
        "\n",
        "print('1994-2022')\n",
        "print(\"True Negative:\", tn)\n",
        "print(\"False Positive:\", fp)\n",
        "print(\"False Negative:\", fn)\n",
        "print(\"True Positive:\", tp)"
      ],
      "metadata": {
        "id": "XgpV1CwjemfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y2022, pred_RegLog)"
      ],
      "metadata": {
        "id": "DvHQ3GTZeyvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y2022, pred_RegLog),\n",
        "                              display_labels=modelo_RegLog.classes_)\n",
        "\n",
        "\n",
        "disp.plot()\n",
        "\n",
        "plt.title('1994-2022')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kNH1EHMIe6TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y2023, modelo_RegLog.predict(X2023)).ravel()\n",
        "\n",
        "print('2023')\n",
        "print(\"True Negative:\", tn)\n",
        "print(\"False Positive:\", fp)\n",
        "print(\"False Negative:\", fn)\n",
        "print(\"True Positive:\", tp)"
      ],
      "metadata": {
        "id": "JFFa3pzFtvPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y2023, modelo_RegLog.predict(X2023)),\n",
        "                              display_labels=modelo_RegLog.classes_)\n",
        "\n",
        "\n",
        "disp.plot()\n",
        "\n",
        "plt.title('2023')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IDk_X1fUfyHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_RegLog_prob2023 = modelo_RegLog.predict_proba(X2023)[:,1]\n",
        "pred_RegLog_prob2023"
      ],
      "metadata": {
        "id": "GDE2jNzVlHfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(15,5))\n",
        "\n",
        "ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023],\n",
        "            pred_RegLog_prob2023*100,\n",
        "            s= 5,\n",
        "            c=pred_RegLog_prob2023*100,\n",
        "            cmap='YlGnBu',\n",
        "            vmin=0,\n",
        "            vmax=100)\n",
        "\n",
        "ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023][y2023 >= 1],\n",
        "            y2023[y2023 >= 1] * 50,\n",
        "            s= 5,\n",
        "            c='r')\n",
        "\n",
        "\n",
        "ax1.set_ylabel('Probabilidad [%]')\n",
        "ax1.set_xlabel('Fecha - UTC')\n",
        "ax1.grid(ls='--', color='grey')\n",
        "\n",
        "plt.suptitle('2023')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oI4gtyVyifUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(15,5))\n",
        "ax1.scatter(df['Date-Time'][(df['Date-Time'].dt.year == 2023).to_numpy()],\n",
        "            df['MAGNITUD Ml'][(df['Date-Time'].dt.year == 2023).to_numpy()],\n",
        "            s=5,\n",
        "            label='Sismos',\n",
        "            alpha=0.5)\n",
        "\n",
        "ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023][modelo_RegLog.predict(X2023) >= 1],\n",
        "            modelo_RegLog.predict(X2023)[modelo_RegLog.predict(X2023) >= 1]*7,\n",
        "            s= 10,\n",
        "            c='g',\n",
        "            label='Predichos Reg Log')\n",
        "\n",
        "\n",
        "ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023][y2023 >= 1],\n",
        "            y2023[y2023 >= 1] * 4.4,\n",
        "            s= 5,\n",
        "            c='r',\n",
        "            label='Datos Y')\n",
        "\n",
        "#ax1.set_ylim(-10, 110)\n",
        "ax1.set_ylabel('Ml')\n",
        "ax1.set_xlabel('Fecha - UTC')\n",
        "ax1.grid(ls='--', color='grey')\n",
        "\n",
        "plt.suptitle('2023')\n",
        "plt.legend(loc=8)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vupML--tp4Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Classifier Model"
      ],
      "metadata": {
        "id": "yv5078PIt5hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X2022.shape"
      ],
      "metadata": {
        "id": "4fqc6TbEvm9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "P5BIIBkkvbaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It seems that when using hl 210, 5 without early_stoping, an overfit is achieved and a score of 1.0 is achieved\n",
        "- When using hl 5, 5 without early_stoping, an overfit is achieved and an average score of .80 is achieved... it only manages to predict 2 for 2023\n",
        "- When using hl 5, 5 with early_stoping, an overfit is achieved and an average score of .70 is achieved... it fails to predict for 2023\n",
        "\n",
        "The early_stopping enables it to consider a part of the data as validation... it is better to continue using early_stopping.\n",
        "```python\n",
        "validation_fractionfloat, default=0.1\n",
        "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.\n",
        "```\n",
        "\n",
        "- In tests on January 22, 2024, a score of 0.90 was achieved and it managed to predict 45 of the events of 2023, hl of 1000, 5 ... with early_stopping ... and considered the data from Day 30 for Y ... this model was unfortunately not saved.\n",
        "\n",
        "Does not predict correctly for 2023:\n",
        "- SD30_ces_a_hl_210_5_0_scr0.789\n",
        "- SD30_ces_a_hl_1000_5_0_scr0.8\n",
        "- SD30_ces_a_hl_5_5_0_scr0.774\n",
        "- SD30_ces_a_hl_1000_5_7_scr0.837\n",
        "- CD30_ces_b_hl_5_5_0_scr0.738\n",
        "\n",
        "They correctly predict for 2023:\n",
        "- SD30_ses_a_hl_210_5_0_scr1.0 ... 15 events\n",
        "- SD30_ses_a_hl_5_5_1_scr0.816 ... 2 events\n",
        "- CD30_ces_a_hl_5_5_6_scr0.804 ... 11 events\n",
        "- CD30_ces_a_hl_5_5_3_scr0.805 ... 10 events\n",
        "- CD30_ces_a_hl_5_5_0_scr0.805 ... 10 events\n",
        "- CD30_ces_b_hl_210_5_0_scr0.842 ... 11 events\n",
        "- CD30_ces_c_hl_210_5_0_scr0.81 ... 8 events\n",
        "- CD30_ces_c_hl_210_5_0_scr0.942 ... 17 events\n",
        "- CD30_ces_b_hl_1000_5_3_scr0.939 ... 13 events\n",
        "- CD30_ces_c_hl_1000_5_2_scr0.901 ... 13\n",
        "- CD30_ces_d_hl_1000_5_5_scr0.908 ... 16\n",
        "- CD30_ces_e_hl_1000_5_3_scr0.932 ... 18\n",
        "- CD30_ces_h_hl_1000_5_4_scr0.955 ... 16\n",
        "- CD30_ces_i_hl_1000_5_2_scr0.912 ... 11\n",
        "- CD30_ces_j_hl_1000_5_3_scr0.974 ... 18\n",
        "- CD30_ces_k_hl_1000_5_8_scr0.908 ... 17\n",
        "- CD30_ces_k_hl_1000_5_0_scr0.9 ... 15\n",
        "- CD30_ces_l_hl_1000_5_2_scr0.908 ... 17\n",
        "\n",
        "Solver= 'sgd'\n",
        "- CD30_ces_f_hl_1000_5_8_scr0.95 ... 16\n",
        "- CD30_ces_f_hl_1000_5_0_scr0.815 ... 6\n",
        "\n",
        "Models with prior 60 days of information will be considered:\n",
        "- CD60_ces_l_hl_1000_5_0_scr0.938.joblib ... 12\n",
        "- CD60_ces_l_hl_1000_5_0_scr0.938 ... 9\n",
        "- CD60_ces_a_hl_1000_5_1_scr0.874 ... 11\n",
        "- CD60_ces_a_hl_1000_5_0_scr0.882...13\n",
        "- CD60_ces_b_hl_1000_5_1_scr0.965 ... 14\n",
        "- CD60_ces_b_hl_1000_5_0_scr0.929 ... 12"
      ],
      "metadata": {
        "id": "RvHjjk0n93dA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'adam' vs 'sgd' ... https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008"
      ],
      "metadata": {
        "id": "0FYVCop0O1Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model is created\n",
        "# there are 210 input data\n",
        "# only the first hidden layer will be modified\n",
        "# the 2nd layer will be left fixed at 5... considering that in the PCA\n",
        "# the presence of 3 Principal Components is inferred\n",
        "# 2 are added for some freedom\n",
        "hl = [2000, 5] # only use 2 layers\n",
        "\n",
        "intentos = 5\n",
        "c = 0\n",
        "\n",
        "# SD30 means that it is a model that the Y did not consider on day 30\n",
        "# CD30 means that it is a model that the Y considers on day 30... the results are better\n",
        "# the additional letter is for multiple runs\n",
        "# ses means without early_stopping\n",
        "# ces means with early_stopping\n",
        "letras = 'SD30_ses_ae'\n",
        "pathSave = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/ModelosMLP_Class/'\n",
        "\n",
        "# For SD30 models the average is about .77, so the one of interest is taken at .80\n",
        "# For CD30 models if they achieve more than .80\n",
        "# The hl 1000, 5 models can achieve the score of .90\n",
        "scoreInteres = 0.7\n",
        "\n",
        "while c <= intentos:\n",
        "  print('\\n ', c)\n",
        "\n",
        "  modelo_MLPClass = MLPClassifier(hidden_layer_sizes=tuple(hl),\n",
        "                                activation='relu', #identity, logistic, tanh, relu\n",
        "                                early_stopping=False, solver='adam', max_iter=30)#, random_state= 0)\n",
        "\n",
        "  # early_stopping is left because when it was not used the model predicts 100%\n",
        "\n",
        "  # It is trained\n",
        "  tinicio = time.perf_counter()\n",
        "\n",
        "  modelo_MLPClass.fit(X2022, y2022)\n",
        "\n",
        "  tfinal = time.perf_counter()\n",
        "  tiempo = tfinal - tinicio\n",
        "\n",
        "  print('Segundos empleados: ',tiempo)\n",
        "\n",
        "  scoreMLPClass = modelo_MLPClass.score(X2022, y2022)\n",
        "  print(scoreMLPClass)\n",
        "\n",
        "  if scoreMLPClass >= scoreInteres:\n",
        "  #if (scoreMLPClass >= scoreInteres) & (scoreMLPClass < 0.91):\n",
        "    # guardar modelo\n",
        "    Name = letras + f'_hl_{hl[0]}_{hl[1]}_' + str(c) + '_scr' + str(round(scoreMLPClass, 3)) + '.joblib'\n",
        "    #dump(modelo_MLPClass, pathSave+Name)\n",
        "    #print(Name)\n",
        "    print('tp 2023', confusion_matrix(y2023, modelo_MLPClass.predict(X2023))[1,1])\n",
        "    if confusion_matrix(y2023, modelo_MLPClass.predict(X2023))[1,1] >=15:\n",
        "      dump(modelo_MLPClass, pathSave+Name)\n",
        "      print(Name)\n",
        "      break\n",
        "\n",
        "  if c == intentos:\n",
        "    print('Nada ...')\n",
        "  c += 1"
      ],
      "metadata": {
        "id": "Z1Jo5EsxuhyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "x_wuAXPIzf05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pathSave = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/ModelosMLP_Class/'\n",
        "Name = 'SD30_ses_a_hl_210_5_0_scr1.0.joblib'\n",
        "modelo_MLPClass = load(pathSave+Name)\n",
        "modelo_MLPClass"
      ],
      "metadata": {
        "id": "7l0SOIV0sCG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(modelo_MLPClass.loss_curve_), '\\n')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(len(modelo_MLPClass.loss_curve_)),\n",
        "         modelo_MLPClass.loss_curve_)\n",
        "\n",
        "plt.xlabel('Iteraciones')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uhQ3YQpbyOe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_MLPClass.classes_"
      ],
      "metadata": {
        "id": "c8SwgD91zSNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predichosMLPClass = modelo_MLPClass.predict(X2022)\n",
        "predProb_MLPClass = modelo_MLPClass.predict_proba(X2022)[:,1]"
      ],
      "metadata": {
        "id": "W3U2UMJDu8JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y2022, predichosMLPClass),\n",
        "                              display_labels=modelo_RegLog.classes_)\n",
        "\n",
        "\n",
        "disp.plot()\n",
        "\n",
        "plt.title('1994-2022 MLP Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-ld6131YvDiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(15,5))\n",
        "\n",
        "ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2022],\n",
        "            predProb_MLPClass*100,\n",
        "            s= 5,\n",
        "            c=predProb_MLPClass*100,\n",
        "            cmap='YlGnBu',\n",
        "            vmin=0,\n",
        "            vmax=100)\n",
        "\n",
        "ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2022][y2022 >= 1],\n",
        "            y2022[y2022 >= 1] * 50,\n",
        "            s= 5,\n",
        "            c='r')\n",
        "\n",
        "\n",
        "ax1.set_ylim(-10, 110)\n",
        "ax1.set_ylabel('Probabilidad [%]')\n",
        "ax1.set_xlabel('Fecha - UTC')\n",
        "ax1.grid(ls='--', color='grey')\n",
        "\n",
        "plt.suptitle('1994-2022 MLP Class')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sKQOg0gDzhgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predecimos\n",
        "predichosMLPClass2023 = modelo_MLPClass.predict(X2023)\n",
        "predProb_MLPClass2023 = modelo_MLPClass.predict_proba(X2023)[:,1]"
      ],
      "metadata": {
        "id": "XUwe0r4jzz14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y2023, modelo_MLPClass.predict(X2023)),\n",
        "                              display_labels=modelo_RegLog.classes_)\n",
        "\n",
        "\n",
        "disp.plot()\n",
        "\n",
        "plt.title('2023 MLP Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lAD54NuwvSMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(15,5))\n",
        "\n",
        "ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023],\n",
        "            predProb_MLPClass2023*100,\n",
        "            s= 5,\n",
        "            c=predProb_MLPClass2023*100,\n",
        "            cmap='YlGnBu',\n",
        "            vmin=0,\n",
        "            vmax=100)\n",
        "\n",
        "ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023][y2023 >= 1],\n",
        "            y2023[y2023 >= 1] * 50,\n",
        "            s= 5,\n",
        "            c='r')\n",
        "\n",
        "ax1.set_ylim(-10, 110)\n",
        "ax1.set_ylabel('Probabilidad [%]')\n",
        "ax1.set_xlabel('Fecha - UTC')\n",
        "ax1.grid(ls='--', color='grey')\n",
        "\n",
        "plt.suptitle('2023 MLP Class')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GnNq6Vwjzr3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(15,5))\n",
        "ax1.scatter(df['Date-Time'][(df['Date-Time'].dt.year == 2023).to_numpy()],\n",
        "            df['MAGNITUD Ml'][(df['Date-Time'].dt.year == 2023).to_numpy()],\n",
        "            s=5,\n",
        "            label='Sismos',\n",
        "            alpha=0.5)\n",
        "\n",
        "ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023][predichosMLPClass2023 >= 1],\n",
        "            predichosMLPClass2023[predichosMLPClass2023 >= 1]*7,\n",
        "            s= 10,\n",
        "            c='g',\n",
        "            label='Predichos MLP Class')\n",
        "\n",
        "\n",
        "ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023][y2023 >= 1],\n",
        "            y2023[y2023 >= 1] * 4.4,\n",
        "            s= 5,\n",
        "            c='r',\n",
        "            label='Datos Y')\n",
        "\n",
        "#ax1.set_ylim(-10, 110)\n",
        "ax1.set_ylabel('Ml')\n",
        "ax1.set_xlabel('Fecha - UTC')\n",
        "ax1.grid(ls='--', color='grey')\n",
        "\n",
        "plt.suptitle('2023')\n",
        "plt.legend(loc=8)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "odLuBvgHYxt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Models"
      ],
      "metadata": {
        "id": "8N15i4xe2BtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model List"
      ],
      "metadata": {
        "id": "jAT6bXRC5YjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "listaModelos = !ls /content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/ModelosMLP_Class\n",
        "listaModelos"
      ],
      "metadata": {
        "id": "1deYFGGs6FMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathModelos = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/ModelosMLP_Class/'\n",
        "listaModelos = !ls /content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/ModelosMLP_Class\n",
        "listaTemp = []\n",
        "for i in range(len(listaModelos)):\n",
        "  if len(listaModelos[i].split('\\t')) == 1:\n",
        "    a = listaModelos[i].split('\\t')\n",
        "    listaTemp.append(a[0])\n",
        "  else:\n",
        "    a = listaModelos[i].split('\\t')\n",
        "    listaTemp.append(a[0])\n",
        "    listaTemp.append(a[1])\n",
        "listaModelos = np.reshape(np.array(listaTemp), (1,-1))\n",
        "del(listaTemp)\n",
        "listaModelos = listaModelos[0]\n",
        "listaModelos"
      ],
      "metadata": {
        "id": "zpHMXPNg2DM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload CSVs"
      ],
      "metadata": {
        "id": "n4Gi70Mt5avp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pathDF = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/Catalogos/'\n",
        "\n",
        "# DataFrame the entire earthquake catalog\n",
        "df = pd.read_csv(pathDF+'df.csv')\n",
        "df['Date-Time'] = pd.to_datetime(df['Date-Time'], yearfirst=True)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "AQQ1VL4q4bTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame by days\n",
        "df_Dias = pd.read_csv(pathDatos+'df_Dias.csv')\n",
        "df_Dias['Fecha'] = pd.to_datetime(df_Dias['Dia'], yearfirst=True)\n",
        "df_Dias"
      ],
      "metadata": {
        "id": "Fpqpy40x43f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dias_Considerar = 30 # 30 o 60\n",
        "nMin = dias_Considerar - 1\n",
        "filtro2023 = (df_Dias['Fecha'].iloc[dias_Considerar-1:-6].dt.year == 2023).to_numpy()"
      ],
      "metadata": {
        "id": "KwyEND609Dvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2023 = np.loadtxt(pathDatos+'SD30_X2023.txt')\n",
        "X2023.shape"
      ],
      "metadata": {
        "id": "hklz-KAluCFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for modelo in listaModelos:\n",
        "  if 'joblib' in modelo:\n",
        "    if 'D30' in modelo:\n",
        "      dias_Considerar = 30\n",
        "      if 'SD' in modelo:\n",
        "        X2023 = np.loadtxt(pathDatos+'SD30_X2023.txt')\n",
        "        y2023 = np.loadtxt(pathDatos+'SD30_y2023.txt')\n",
        "        X2022 = np.loadtxt(pathDatos+'SD30_X2022.txt')\n",
        "        y2022 = np.loadtxt(pathDatos+'SD30_y2022.txt')\n",
        "\n",
        "      if 'CD' in modelo:\n",
        "        X2023 = np.loadtxt(pathDatos+'CD30_X2023.txt')\n",
        "        y2023 = np.loadtxt(pathDatos+'CD30_y2023.txt')\n",
        "        X2022 = np.loadtxt(pathDatos+'CD30_X2022.txt')\n",
        "        y2022 = np.loadtxt(pathDatos+'CD30_y2022.txt')\n",
        "\n",
        "    if 'D60' in modelo:\n",
        "      dias_Considerar = 60\n",
        "      if 'SD' in modelo:\n",
        "        X2023 = np.loadtxt(pathDatos+'SD60_X2023.txt')\n",
        "        y2023 = np.loadtxt(pathDatos+'SD60_y2023.txt')\n",
        "        X2022 = np.loadtxt(pathDatos+'SD60_X2022.txt')\n",
        "        y2022 = np.loadtxt(pathDatos+'SD60_y2022.txt')\n",
        "      if 'CD' in modelo:\n",
        "        X2023 = np.loadtxt(pathDatos+'CD60_X2023.txt')\n",
        "        y2023 = np.loadtxt(pathDatos+'CD60_y2023.txt')\n",
        "        X2022 = np.loadtxt(pathDatos+'CD60_X2022.txt')\n",
        "        y2022 = np.loadtxt(pathDatos+'CD60_y2022.txt')\n",
        "\n",
        "    modelo_P = load(pathModelos+modelo)\n",
        "    # 2023\n",
        "    predichosModelo = modelo_P.predict(X2023)\n",
        "    predProb_Modelo = modelo_P.predict_proba(X2023)[:,1]\n",
        "    # 1994-2023\n",
        "    #predichosModelo_2022 = modelo_P.predict(X2022)\n",
        "    predProb_Modelo_2022 = modelo_P.predict_proba(X2022)[:,1]\n",
        "\n",
        "    nMin = dias_Considerar - 1\n",
        "    filtro2023 = (df_Dias['Fecha'].iloc[dias_Considerar-1:-6].dt.year == 2023).to_numpy()\n",
        "    filtro2022 = (df_Dias['Fecha'].iloc[dias_Considerar-1:-6].dt.year < 2023).to_numpy()\n",
        "\n",
        "\n",
        "    # Figura entrenamiento 1994-2022\n",
        "    fig = plt.figure(figsize=(15,5), constrained_layout=True)\n",
        "    gs = fig.add_gridspec(1,3)\n",
        "\n",
        "    if 'RegLog' in modelo:\n",
        "      ax1 = fig.add_subplot(gs[0, 0:3])\n",
        "      ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2022],\n",
        "                  predProb_Modelo_2022*100,\n",
        "                  s= 5,\n",
        "                  c=predProb_Modelo_2022*100,\n",
        "                  cmap='YlGnBu',\n",
        "                  vmin=0,\n",
        "                  vmax=100)\n",
        "\n",
        "      ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2022][y2022 >= 1],\n",
        "                  y2022[y2022 >= 1] * 50,\n",
        "                  s= 5,\n",
        "                  c='r')\n",
        "      ax1.set_ylim(-10, 110)\n",
        "      ax1.set_ylabel('Probabilidad [%]')\n",
        "      ax1.set_xlabel('Fecha - UTC')\n",
        "      ax1.grid(ls='--', color='grey')\n",
        "\n",
        "    else:\n",
        "      ax1 = fig.add_subplot(gs[0, 0:2])\n",
        "      ax2 = fig.add_subplot(gs[0, 2])\n",
        "\n",
        "      ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2022],\n",
        "                  predProb_Modelo_2022*100,\n",
        "                  s= 5,\n",
        "                  c=predProb_Modelo_2022*100,\n",
        "                  cmap='YlGnBu',\n",
        "                  vmin=0,\n",
        "                  vmax=100)\n",
        "\n",
        "      ax1.scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2022][y2022 >= 1],\n",
        "                  y2022[y2022 >= 1] * 50,\n",
        "                  s= 5,\n",
        "                  c='r')\n",
        "      ax1.set_ylim(-10, 110)\n",
        "      ax1.set_ylabel('Probabilidad [%]')\n",
        "      ax1.set_xlabel('Fecha - UTC')\n",
        "      ax1.grid(ls='--', color='grey')\n",
        "\n",
        "      ax2.plot(np.arange(len(modelo_P.loss_curve_)),\n",
        "              modelo_P.loss_curve_)\n",
        "      ax2.grid(ls='--', color='grey')\n",
        "      ax2.set_xlabel('Iteraciones')\n",
        "      ax2.set_ylabel('Loss')\n",
        "\n",
        "    plt.suptitle(f'Entrenamiento 1994-2022 Modelo: {modelo[:-7]}')\n",
        "    plt.savefig((pathSaveFiguras + f'1994_2022_{modelo[:-7]}.png'),\n",
        "                format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "    plt.show()\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "    # Figuras 2023\n",
        "    fig, ax = plt.subplots(2,1, figsize=(15,10), sharex=True)\n",
        "    fig.subplots_adjust(hspace=0.1)\n",
        "\n",
        "    # Probabilidad\n",
        "    ax[0].scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023],\n",
        "                predProb_Modelo*100,\n",
        "                s= 5,\n",
        "                c=predProb_Modelo*100,\n",
        "                cmap='YlGnBu',\n",
        "                vmin=0,\n",
        "                vmax=100)\n",
        "\n",
        "    ax[0].scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023][y2023 >= 1],\n",
        "                y2023[y2023 >= 1] * 50,\n",
        "                s= 5,\n",
        "                c='r')\n",
        "\n",
        "    ax[0].set_ylim(-10, 110)\n",
        "    ax[0].set_ylabel('Probabilidad [%]')\n",
        "    #ax[0].set_xlabel('Fecha - UTC')\n",
        "    ax[0].grid(ls='--', color='grey')\n",
        "    ax[0].set_title(f'Predicción 2023 Modelo: {modelo[:-7]}')\n",
        "\n",
        "\n",
        "\n",
        "    # Eventos logrados\n",
        "    event4_5 = df['MAGNITUD Ml'][(df['Date-Time'].dt.year == 2023).to_numpy()]\n",
        "    event4_5 = event4_5 >= 4.5\n",
        "    event4_5 = sum(event4_5)\n",
        "    ax[1].scatter(df['Date-Time'][(df['Date-Time'].dt.year == 2023).to_numpy()],\n",
        "                df['MAGNITUD Ml'][(df['Date-Time'].dt.year == 2023).to_numpy()],\n",
        "                s=5,\n",
        "                label=f'Sismos, {event4_5} eventos Ml >=4.5 ',\n",
        "                alpha=0.5)\n",
        "\n",
        "    ax[1].scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023][predichosModelo >= 1],\n",
        "                predichosModelo[predichosModelo >= 1]*7,\n",
        "                s= 10,\n",
        "                c='g',\n",
        "                label=f'Predichos: {sum(predichosModelo)}')\n",
        "\n",
        "\n",
        "    ax[1].scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023][y2023 >= 1],\n",
        "                y2023[y2023 >= 1] * 4.4,\n",
        "                s= 5,\n",
        "                c='r',\n",
        "                label='Datos Y')\n",
        "\n",
        "    ax[1].set_ylim(-0.5, 7.5)\n",
        "    ax[1].set_ylabel('Ml')\n",
        "    ax[1].set_xlabel('Fecha - UTC')\n",
        "    ax[1].grid(ls='--', color='grey')\n",
        "\n",
        "\n",
        "    plt.legend(loc=8)\n",
        "\n",
        "    plt.savefig((pathSaveFiguras + f'2023_{modelo[:-7]}.png'),\n",
        "                format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "ciGk5VNl9unc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2022"
      ],
      "metadata": {
        "id": "ZseBfCAoqkkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for modelo in listaModelos:\n",
        "  if 'joblib' in modelo:\n",
        "    if 'D30' in modelo:\n",
        "      dias_Considerar = 30\n",
        "      if 'SD' in modelo:\n",
        "        X2023 = np.loadtxt(pathDatos+'SD30_X2023.txt')\n",
        "        y2023 = np.loadtxt(pathDatos+'SD30_y2023.txt')\n",
        "        X2022 = np.loadtxt(pathDatos+'SD30_X2022.txt')\n",
        "        y2022 = np.loadtxt(pathDatos+'SD30_y2022.txt')\n",
        "\n",
        "      if 'CD' in modelo:\n",
        "        X2023 = np.loadtxt(pathDatos+'CD30_X2023.txt')\n",
        "        y2023 = np.loadtxt(pathDatos+'CD30_y2023.txt')\n",
        "        X2022 = np.loadtxt(pathDatos+'CD30_X2022.txt')\n",
        "        y2022 = np.loadtxt(pathDatos+'CD30_y2022.txt')\n",
        "\n",
        "    if 'D60' in modelo:\n",
        "      dias_Considerar = 60\n",
        "      if 'SD' in modelo:\n",
        "        X2023 = np.loadtxt(pathDatos+'SD60_X2023.txt')\n",
        "        y2023 = np.loadtxt(pathDatos+'SD60_y2023.txt')\n",
        "        X2022 = np.loadtxt(pathDatos+'SD60_X2022.txt')\n",
        "        y2022 = np.loadtxt(pathDatos+'SD60_y2022.txt')\n",
        "      if 'CD' in modelo:\n",
        "        X2023 = np.loadtxt(pathDatos+'CD60_X2023.txt')\n",
        "        y2023 = np.loadtxt(pathDatos+'CD60_y2023.txt')\n",
        "        X2022 = np.loadtxt(pathDatos+'CD60_X2022.txt')\n",
        "        y2022 = np.loadtxt(pathDatos+'CD60_y2022.txt')\n",
        "\n",
        "    modelo_P = load(pathModelos+modelo)\n",
        "    # 2023\n",
        "    predichosModelo = modelo_P.predict(X2022)[-365:]\n",
        "    predProb_Modelo = modelo_P.predict_proba(X2022)[:,1]\n",
        "\n",
        "    nMin = dias_Considerar - 1\n",
        "    filtro2022 = (df_Dias['Fecha'].iloc[dias_Considerar-1:-6].dt.year == 2022).to_numpy()\n",
        "\n",
        "    # Figuras 2022\n",
        "    fig, ax = plt.subplots(2,1, figsize=(15,10), sharex=True)\n",
        "    fig.subplots_adjust(hspace=0.1)\n",
        "\n",
        "    # Probabilidad\n",
        "    ax[0].scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2022],\n",
        "                predProb_Modelo[-365:]*100,\n",
        "                s= 5,\n",
        "                c=predProb_Modelo[-365:]*100,\n",
        "                cmap='YlGnBu',\n",
        "                vmin=0,\n",
        "                vmax=100)\n",
        "\n",
        "\n",
        "    y2022 = y2022[-365:]\n",
        "    ax[0].scatter(df_Dias['Fecha'].iloc[dias_Considerar-1:-6][filtro2022][y2022 >= 1],\n",
        "                y2022[y2022 >= 1] * 50,\n",
        "                s= 5,\n",
        "                c='r')\n",
        "\n",
        "    ax[0].set_ylim(-10, 110)\n",
        "    ax[0].set_ylabel('Probabilidad [%]')\n",
        "    #ax[0].set_xlabel('Fecha - UTC')\n",
        "    ax[0].grid(ls='--', color='grey')\n",
        "    ax[0].set_title(f'Predicción 2022 Modelo: {modelo[:-7]}')\n",
        "\n",
        "\n",
        "    # Eventos logrados\n",
        "    event4_5 = df['MAGNITUD Ml'][(df['Date-Time'].dt.year == 2022).to_numpy()]\n",
        "    event4_5 = event4_5 >= 4.5\n",
        "    event4_5 = sum(event4_5)\n",
        "    ax[1].scatter(df['Date-Time'][(df['Date-Time'].dt.year == 2022).to_numpy()],\n",
        "                df['MAGNITUD Ml'][(df['Date-Time'].dt.year == 2022).to_numpy()],\n",
        "                s=5,\n",
        "                label=f'Sismos, {event4_5} eventos Ml >=4.5 ',\n",
        "                alpha=0.5)\n",
        "\n",
        "    ax[1].scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2022][predichosModelo >= 1],\n",
        "                predichosModelo[predichosModelo >= 1]*7,\n",
        "                s= 10,\n",
        "                c='g',\n",
        "                label=f'Predichos: {sum(predichosModelo)}')\n",
        "\n",
        "\n",
        "    ax[1].scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2022][y2022 >= 1],\n",
        "                y2022[y2022 >= 1] * 4.4,\n",
        "                s= 5,\n",
        "                c='r',\n",
        "                label='Datos Y')\n",
        "\n",
        "    ax[1].set_ylim(-0.5, 7.5)\n",
        "    ax[1].set_ylabel('Ml')\n",
        "    ax[1].set_xlabel('Fecha - UTC')\n",
        "    ax[1].grid(ls='--', color='grey')\n",
        "\n",
        "\n",
        "    plt.legend(loc=8)\n",
        "\n",
        "    plt.savefig((pathSaveFiguras + f'2022_{modelo[:-7]}.png'),\n",
        "                format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "CUscdQzyqmQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2024\n"
      ],
      "metadata": {
        "id": "dXbR1S7gADLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start"
      ],
      "metadata": {
        "id": "EZXGx3IBA03k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "pathDatos = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/Catalogos/'\n",
        "pathSaveFiguras = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/Figuras/'\n",
        "pathDF = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/Catalogos/'"
      ],
      "metadata": {
        "id": "hRmD6s8aAGmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load catalog 2024"
      ],
      "metadata": {
        "id": "oF3pBVROA4gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(pathDatos+'EneroFebrero_2024.xlsx', decimal=',')\n",
        "df.info()"
      ],
      "metadata": {
        "id": "_CwOHpAy_o6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only those greater than 50 km deep\n",
        "df_2024 = df[df['PROF. (Km)'] >= 50].reset_index(drop=True)\n",
        "df_2024.info()"
      ],
      "metadata": {
        "id": "qAxiLh2yAB0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only magnitudes greater than 2.5\n",
        "df_2024 = df_2024[df_2024['MAGNITUD'] >= 2.5].reset_index(drop=True)\n",
        "df_2024.info()"
      ],
      "metadata": {
        "id": "_GvZQRFlAabK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## df_Dias 2024"
      ],
      "metadata": {
        "id": "dVo5FPD6A8Ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024.head()"
      ],
      "metadata": {
        "id": "rBvaaS_LBIsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024[['FECHA', 'HORA_UTC']] = df_2024['FECHA - HORA UTC'].str.split(' ', expand=True)\n",
        "df_2024['Fecha'] = pd.to_datetime(df_2024['FECHA'], yearfirst=True)\n",
        "df_2024.info()"
      ],
      "metadata": {
        "id": "NNTvD8HVBRvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024.tail()"
      ],
      "metadata": {
        "id": "x5CaawWmDfdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024['Date-Time'] = pd.to_datetime(df_2024['FECHA - HORA UTC'], yearfirst=True)"
      ],
      "metadata": {
        "id": "5euJ3Gl7O6sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2024.columns"
      ],
      "metadata": {
        "id": "XCeIFyBxNag2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save dfTotalNuevo"
      ],
      "metadata": {
        "id": "i27ZMkoZN9OT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[['FECHA', 'HORA_UTC']] = df['FECHA - HORA UTC'].str.split(' ', expand=True)\n",
        "df['Date-Time'] = pd.to_datetime(df['FECHA - HORA UTC'], yearfirst=True)"
      ],
      "metadata": {
        "id": "AVpy3h4PVUPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2_ = df[['FECHA', 'HORA_UTC','LATITUD (°)', 'LONGITUD (°)', 'PROF. (Km)',\n",
        "                'MAGNITUD', 'ERROR LATITUD (Km)', 'ERROR LONGITUD (Km)', 'ERROR PROFUNDIDAD (Km)', 'Date-Time']].copy()\n",
        "df2_.columns = ['FECHA', 'HORA_UTC', 'LATITUD (grados)', 'LONGITUD (grados)',\n",
        "                'PROFUNDIDAD (Km)', 'MAGNITUD Ml', 'ERROR LATITUD (Km)',\n",
        "                'ERROR LONGITUD (Km)', 'ERROR PROFUNDIDAD (Km)', 'Date-Time']\n",
        "df2_"
      ],
      "metadata": {
        "id": "KOZwu_8sNfP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df antiguo total\n",
        "pathDF = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/Catalogos/'\n",
        "df = pd.read_csv(pathDF+'df.csv')\n",
        "#df['Date-Time'] = pd.to_datetime(df['Date-Time'], yearfirst=True)\n",
        "df.columns"
      ],
      "metadata": {
        "id": "LqsfwfbENPqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Total_1994_2024 = pd.concat([df, df2_], ignore_index=True)\n",
        "df_Total_1994_2024.reset_index(drop=True)\n",
        "df_Total_1994_2024"
      ],
      "metadata": {
        "id": "VxG_w-2rOZDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Total_1994_2024.to_csv(pathDatos+'df_Total_1994_2024.csv', index=False)"
      ],
      "metadata": {
        "id": "gpdfEfutPLUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create df New Days"
      ],
      "metadata": {
        "id": "HSABoQvhOZha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_Dias = {'Dia':[],\n",
        "             '2.5-3.0':[],\n",
        "             '3.0-3.5':[],\n",
        "             '3.5-4.0':[],\n",
        "             '4.0-4.5':[],\n",
        "             '4.5-5.0':[],\n",
        "             '5.0-5.5':[],\n",
        "             '5.5-':[],}\n",
        "\n",
        "# Apparently not every day there is a record of events with depths greater\n",
        "# than 50 km and magnitudes greater than 2.5\n",
        "inicio = np.datetime64('2024-01-01')\n",
        "fin = np.datetime64('2024-03-01')\n",
        "array_fechas = np.arange(inicio, fin, dtype='datetime64[D]')\n",
        "\n",
        "for date in array_fechas:\n",
        "  df_filtrado = df_2024[df_2024['FECHA'] == str(date)]\n",
        "  dict_Dias['Dia'].append(str(date))\n",
        "\n",
        "  for ml in np.arange(2.5, 6.0, 0.5):\n",
        "    if ml == 5.5:\n",
        "      nE = df_filtrado['MAGNITUD'] >= 5.5\n",
        "\n",
        "      dict_Dias['5.5-'].append(sum(nE))\n",
        "\n",
        "    else:\n",
        "      nE = (df_filtrado['MAGNITUD'] >= ml) & (df_filtrado['MAGNITUD'] < (ml+0.5) )\n",
        "      strDict = f'{ml}-{ml+0.5}'\n",
        "\n",
        "      dict_Dias[strDict].append(sum(nE))\n",
        "\n",
        "df_Dias2024 = pd.DataFrame.from_dict(dict_Dias)\n",
        "df_Dias2024['Total dia'] = df_Dias2024.sum(axis=1)\n",
        "df_Dias2024"
      ],
      "metadata": {
        "id": "rcgigHhnA-Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load old df_Dias"
      ],
      "metadata": {
        "id": "nqe2jBS5EgVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the df_Dias to then join with 2024\n",
        "df_Dias = pd.read_csv(pathDatos+'df_Dias.csv')\n",
        "df_Dias"
      ],
      "metadata": {
        "id": "Rwvae8BU_X_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias_nuevo = pd.concat([df_Dias, df_Dias2024], ignore_index=True)\n",
        "df_Dias_nuevo.reset_index(drop=True)\n",
        "df_Dias_nuevo['Fecha'] = pd.to_datetime(df_Dias_nuevo['Dia'], yearfirst=True)\n",
        "df_Dias_nuevo"
      ],
      "metadata": {
        "id": "uvrbER8wEsdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias_nuevo.info()"
      ],
      "metadata": {
        "id": "u2ytGRQwFzUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save CSV New Days"
      ],
      "metadata": {
        "id": "8v15fGo8P2WR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias_nuevo[['Dia', '2.5-3.0', '3.0-3.5', '3.5-4.0', '4.0-4.5', '4.5-5.0', '5.0-5.5',\n",
        "               '5.5-', 'Total dia', 'Fecha']].to_csv(pathDatos+'df_Dias_1994_2024.csv', index=False)"
      ],
      "metadata": {
        "id": "F7y6Uir0P6yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Dias_nuevo['Y'] = df_Dias_nuevo[['4.5-5.0', '5.0-5.5', '5.5-']].sum(axis=1)\n",
        "np.round(df_Dias_nuevo.describe())"
      ],
      "metadata": {
        "id": "38qQpIkmHXvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create x/y 2023-2024"
      ],
      "metadata": {
        "id": "20_NpvH7Gpsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = []\n",
        "X = []\n",
        "\n",
        "dias_Considerar = 60\n",
        "fechaInicio = datetime(2023,1,1) - timedelta(days = dias_Considerar-1)\n",
        "df_Dias = df_Dias_nuevo[df_Dias_nuevo['Fecha'] >= fechaInicio].copy()\n",
        "\n",
        "for dias in range(dias_Considerar-1, len(df_Dias)-6):\n",
        "  di_ = dias_Considerar-1\n",
        "  datos = df_Dias[['2.5-3.0', '3.0-3.5', '3.5-4.0', '4.0-4.5', '4.5-5.0', '5.0-5.5', '5.5-']].iloc[dias-di_:dias+1].to_numpy()\n",
        "  #print(datos.shape)\n",
        "  datos = np.reshape(datos, (1,-1))\n",
        "  #print(datos.shape)\n",
        "  X.append(datos.tolist()[0])\n",
        "\n",
        "  # Si no se incluye el día 30 en la consulta para considerar en los targets\n",
        "  # en el entrenamiento el modelo no predice\n",
        "  # porque?\n",
        "  # SD30 ... sin el día 30\n",
        "  # SD60 ... sin el día 60\n",
        "  target = df_Dias['Y'].iloc[dias+1:dias+7].sum()\n",
        "\n",
        "  # Si se incluye el día 30, el modelo si predice mejor\n",
        "  # porque?\n",
        "  # CD30 ... con el día 30\n",
        "  # CD60 ... con el día 60\n",
        "  #target = df_Dias['Y'].iloc[dias:dias+7].sum()\n",
        "\n",
        "  if target > 0 :\n",
        "    y.append(1)\n",
        "  else:\n",
        "    y.append(0)\n",
        "y = np.reshape(np.array(y), (-1,1))\n",
        "X = np.array(X)"
      ],
      "metadata": {
        "id": "M5_X_wNPGnNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "WVOATEtSJZ1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "QO8au44BJcy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_2024 = pd.DataFrame(np.concatenate((X,y), axis=1))\n",
        "df_2023_2024"
      ],
      "metadata": {
        "id": "tJw7yYMrJ1Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save/Load df_2023_2024"
      ],
      "metadata": {
        "id": "_aoiMX2oJ-h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_2024.to_csv(pathDatos+'df_2023_2024.csv', index=False)"
      ],
      "metadata": {
        "id": "b3yvRoslKBzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_2024 = pd.read_csv(pathDatos+'df_2023_2024.csv')\n",
        "df_2023_2024.head()"
      ],
      "metadata": {
        "id": "e76F5yyUKMgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y2024 = df_2023_2024.iloc[:,-1].to_numpy()\n",
        "X2024 = df_2023_2024.iloc[:,:-1].to_numpy()"
      ],
      "metadata": {
        "id": "oCdqk79XK0LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefijo = pathDatos + 'SD' + str(dias_Considerar)\n",
        "print(prefijo)\n",
        "np.savetxt(f'{prefijo}_y2024.txt', y2024)\n",
        "np.savetxt(f'{prefijo}_X2024.txt', X2024)"
      ],
      "metadata": {
        "id": "vtH87zw4Kjae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparar 2024"
      ],
      "metadata": {
        "id": "V73G6jbJL6CG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "listaModelos = !ls /content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/ModelosMLP_Class\n",
        "listaModelos"
      ],
      "metadata": {
        "id": "0UyNG61-L79E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathModelos = '/content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/ModelosMLP_Class/'\n",
        "listaModelos = !ls /content/drive/MyDrive/Manuscritos_Investigacion/ML_SismosNidoBucaramanga/ModelosMLP_Class\n",
        "listaTemp = []\n",
        "for i in range(len(listaModelos)):\n",
        "  if len(listaModelos[i].split('\\t')) == 1:\n",
        "    a = listaModelos[i].split('\\t')\n",
        "    listaTemp.append(a[0])\n",
        "  else:\n",
        "    a = listaModelos[i].split('\\t')\n",
        "    listaTemp.append(a[0])\n",
        "    listaTemp.append(a[1])\n",
        "listaModelos = np.reshape(np.array(listaTemp), (1,-1))\n",
        "del(listaTemp)\n",
        "listaModelos = listaModelos[0]\n",
        "listaModelos"
      ],
      "metadata": {
        "id": "xv4b3FjvMAc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load df Total"
      ],
      "metadata": {
        "id": "5WeCmpvYPWNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(pathDF+'df_Total_1994_2024.csv')\n",
        "df['Date-Time'] = pd.to_datetime(df['Date-Time'], yearfirst=True)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "6GRqAkUMMdoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load df Days Total"
      ],
      "metadata": {
        "id": "prvimeSZPpGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame by days\n",
        "df_Dias = pd.read_csv(pathDatos+'df_Dias_1994_2024.csv')\n",
        "df_Dias['Fecha'] = pd.to_datetime(df_Dias['Dia'], yearfirst=True)\n",
        "df_Dias"
      ],
      "metadata": {
        "id": "sczu__DyPsEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots"
      ],
      "metadata": {
        "id": "1psjLtOMRCj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for modelo in listaModelos:\n",
        "  if 'joblib' in modelo:\n",
        "    if 'D30' in modelo:\n",
        "      dias_Considerar = 30\n",
        "      if 'SD' in modelo:\n",
        "        X2023 = np.loadtxt(pathDatos+'SD30_X2024.txt')\n",
        "        y2023 = np.loadtxt(pathDatos+'SD30_y2024.txt')\n",
        "\n",
        "      if 'CD' in modelo:\n",
        "        X2023 = np.loadtxt(pathDatos+'CD30_X2024.txt')\n",
        "        y2023 = np.loadtxt(pathDatos+'CD30_y2024.txt')\n",
        "\n",
        "\n",
        "    if 'D60' in modelo:\n",
        "      dias_Considerar = 60\n",
        "      if 'SD' in modelo:\n",
        "        X2023 = np.loadtxt(pathDatos+'SD60_X2024.txt')\n",
        "        y2023 = np.loadtxt(pathDatos+'SD60_y2024.txt')\n",
        "\n",
        "      if 'CD' in modelo:\n",
        "        X2023 = np.loadtxt(pathDatos+'CD60_X2024.txt')\n",
        "        y2023 = np.loadtxt(pathDatos+'CD60_y2024.txt')\n",
        "\n",
        "\n",
        "    nMin = dias_Considerar - 1\n",
        "    filtro2023 = (df_Dias['Fecha'].iloc[dias_Considerar-1:-6].dt.year >= 2023).to_numpy()\n",
        "\n",
        "    modelo_P = load(pathModelos+modelo)\n",
        "    # 2023\n",
        "    predichosModelo = modelo_P.predict(X2023)\n",
        "    predProb_Modelo = modelo_P.predict_proba(X2023)[:,1]\n",
        "\n",
        "\n",
        "    nMin = dias_Considerar - 1\n",
        "    filtro2023 = (df_Dias['Fecha'].iloc[dias_Considerar-1:-6].dt.year >= 2023).to_numpy()\n",
        "\n",
        "    # Figuras 2023-2024\n",
        "    fig, ax = plt.subplots(2,1, figsize=(15,10), sharex=True)\n",
        "    fig.subplots_adjust(hspace=0.1)\n",
        "\n",
        "    # Probabilidad\n",
        "    print(df_Dias['Fecha'].iloc[nMin:-6][filtro2023].shape)\n",
        "    print(len(predProb_Modelo))\n",
        "    ax[0].scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023],\n",
        "                predProb_Modelo*100,\n",
        "                s= 5,\n",
        "                c=predProb_Modelo*100,\n",
        "                cmap='YlGnBu',\n",
        "                vmin=0,\n",
        "                vmax=100)\n",
        "\n",
        "    ax[0].scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023][y2023 >= 1],\n",
        "                y2023[y2023 >= 1] * 50,\n",
        "                s= 5,\n",
        "                c='r')\n",
        "\n",
        "    ax[0].set_ylim(-10, 110)\n",
        "    ax[0].set_ylabel('Probabilidad [%]')\n",
        "    #ax[0].set_xlabel('Fecha - UTC')\n",
        "    ax[0].grid(ls='--', color='grey')\n",
        "    ax[0].set_title(f'Predicción 2023 Modelo: {modelo[:-7]}')\n",
        "\n",
        "\n",
        "\n",
        "    # Eventos logrados\n",
        "    event4_5 = df['MAGNITUD Ml'][(df['Date-Time'].dt.year >= 2023).to_numpy()]\n",
        "    event4_5 = event4_5 >= 4.5\n",
        "    event4_5 = sum(event4_5)\n",
        "    ax[1].scatter(df['Date-Time'][(df['Date-Time'].dt.year >= 2023).to_numpy()],\n",
        "                df['MAGNITUD Ml'][(df['Date-Time'].dt.year >= 2023).to_numpy()],\n",
        "                s=5,\n",
        "                label=f'Sismos, {event4_5} eventos Ml >=4.5 ',\n",
        "                alpha=0.5)\n",
        "\n",
        "    ax[1].scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023][predichosModelo >= 1],\n",
        "                predichosModelo[predichosModelo >= 1]*7,\n",
        "                s= 10,\n",
        "                c='g',\n",
        "                label=f'Predichos: {sum(predichosModelo)}')\n",
        "\n",
        "\n",
        "    ax[1].scatter(df_Dias['Fecha'].iloc[nMin:-6][filtro2023][y2023 >= 1],\n",
        "                y2023[y2023 >= 1] * 4.4,\n",
        "                s= 5,\n",
        "                c='r',\n",
        "                label='Datos Y')\n",
        "\n",
        "    ax[1].set_ylim(-0.5, 7.5)\n",
        "    ax[1].set_ylabel('Ml')\n",
        "    ax[1].set_xlabel('Fecha - UTC')\n",
        "    ax[1].grid(ls='--', color='grey')\n",
        "\n",
        "\n",
        "    plt.legend(loc=8)\n",
        "\n",
        "    plt.savefig((pathSaveFiguras + f'2024_{modelo[:-7]}.png'),\n",
        "                format='png', dpi=300, bbox_inches = 'tight',pad_inches=0.25)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "U1i7k9dORBiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "dDtcqKvcunlr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A38ga-K74Ah7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}